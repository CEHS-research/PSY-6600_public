---
title: "Linear Regression"
subtitle: "Cohen Chapter 10 <br><br> .small[EDUC/PSY 6600]"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: pres2.css
    nature:
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
      ratio: '16:9'
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(comment     = NA,
                      cache       = TRUE,
                      echo        = TRUE, 
                      warning     = FALSE, 
                      message     = FALSE,
                      fig.align   = "center",   # center all figures
                      fig.width   = 6.5,        # set default figure width to 4 inches
                      fig.height  = 4.5)        # set default figure height to 3 inches
```

class: center, middle

## Fit the analysis to the data, 
## *not* the data to the analysis.

#### - Statistical Maxim

---
## Motivating Example

.large[
> Dr. Ramsey conducts a .bluer[**non-experimental (observational) study**] to evaluate what she refers to as the 'strength-injury hypothesis.'   
]

It states that overall body strength in elderly women determines the number and severity of accidents that cause bodily injury. If the results support her hypothesis, she plans to conduct an .bluer[**experimental (randomized) study**] to assess whether weight training reduces injuries in elderly women.   

--

- Data from .nicegreen[100 women] who range in age from 60 to 70 years old are collected.   

- The women initially undergo a series of measures that assess upper and lower body strength.  These measures are summarized into an .nicegreen[**overall index of body strength**]. 

- Over the next 5 years, the women record each time they have an accident that results in a bodily injury and describe fully the extent of the injury.  On the basis of these data, Dr. Ramsey calculates an .nicegreen[**overall injury index**] for each woman. 

--

.large[
> A .dcoral[**simple regression analysis**] is conducted with the .nicegreen[overall index of body strength] as the **predictor (independent) variable** and the .nicegreen[overall injury index] as the **outcome (dependent) variable**. 
]


---

.pull-left[

## .nicegreen[Correlation]

> Chapter 9

**Pairwise Relationship**

- Variable order doesn't matter   
- No DV or IV, just a pair of variables

**Symbols**
- Population Parameter = $\rho$   
- Sample Statistic = $r$

**Language**
- *Evidence the $X_1$ and $X_2$ are .nicegreen[associated]*
- *There is a .nicegreen[correlation] between $X_1$ and $X_2$*

]

--

.pull-right[


## .dcoral[Regression]

> Chapter 10

**Directional Predictive**

- Y = Outcome, Dependent Variable (DV)    
- X = Predictor, Independent Variable (IV)   

**Symbols**
- Population Parameter = $\beta_1$   
- Sample Statistic = $b_1$

**Language**
- *.dcoral[Predicting] Y from X*
- *.dcoral[Regressing] Y on X*

]





---

<!-- 365 Data Science: The Differences Between Correlation and Regression (5 min)-->

<iframe width="1000" height="750" src="https://www.youtube.com/embed/qA-x72ju2kM?controls=0&amp;start=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>




---
## The Line of Best Fit

- The simple, linear relationship is usually not perfect *(points off the line)*
- Regression coefficients .nicegreen[( $b_0$ , $b_1$ )] computed to **minimize error** as much as possible



--

.pull-left[

**Residuals = Error**
- The difference between observed $\large Y$ and $\large \hat{Y}$ 
- Residuals: $\large e_i = Y_i - \hat{Y}_i$


**Technique**

- Ordinary Least Squares (OLS) regression

**Goal**

- Minimize $\large SS_{error}$ ( $\large SS_{residuals}$ )

$$\large SS_{residuals} = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2$$

]


--

.pull-right[
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=5, fig.width=5, dpi = 150, out.width = "80%"}
set.seed(843)
library(tidyverse)
df <- tibble::data_frame(
  x = rnorm(100),
  y = x + rnorm(100)
) %>%
  mutate(pred = predict(lm(y ~ x, data = .)))

df %>%
  ggplot(aes(x, y)) +
    geom_point() +
    geom_smooth(method = "lm", 
                color = "dodgerblue4",
                se = FALSE) +
    theme_bw()
```

]

---
count: false

## The Line of Best Fit

- The simple, linear relationship is usually not perfect *(points off the line)*
- Regression coefficients .nicegreen[( $b_0$ , $b_1$ )] computed to **minimize error** as much as possible


.pull-left[

**Residuals = Error**
- The difference between observed $\large Y$ and $\large \hat{Y}$ 
- Residuals: $\large e_i = Y_i - \hat{Y}_i$


**Technique**

- Ordinary Least Squares (OLS) regression

**Goal**

- Minimize $\large SS_{error}$ ( $\large SS_{residuals}$ )

$$\large SS_{residuals} = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2$$

]



.pull-right[
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=5, fig.width=5, dpi = 150, out.width = "80%"}
df %>%
  ggplot(aes(x, y)) +
    geom_segment(aes(xend = x, yend = pred, color = y - pred, alpha = abs(y - pred))) +
    geom_point() +
    geom_smooth(method = "lm", 
                color = "dodgerblue4",
                se = FALSE,
                size = 2,
                alpha = .8) +
    theme_bw() +
    scale_color_gradient2(low = "chartreuse3", 
                          mid = "grey90", 
                          high = "dodgerblue3") +
  labs(color = "Size and\nDirection of\nResidual",
       alpha = "Size and\nDirection of\nResidual") +
  theme(legend.position = "none")
```

]




---

<!-- Sketchy EBM: Simple Regression (8 min)-->

<iframe width="1000" height="750" src="https://www.youtube.com/embed/SYY_BPciXPw?controls=0&amp;start=12" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>





---
## The Regression Equation

.coral[

$$ \LARGE \hat{Y_i} = b_0 + b_1 X_i $$
]

--


.center[

$\Large X_i$ = value of predictor for a given case i

$\Large \hat{Y_i}$ = predicted (unobserved) value of Y for a given case i

]


--

.pull-left[
.bluer[
.large[Y-Intercept]

- Predicted $\hat{Y}$, when X = 0   

- Interpreted only if $X = 0$ is meaningful


$$\LARGE
b_0 = \bar{Y} - b_1 \times \bar{X}
$$

]]


--

.pull-right[
.nicegreen[
.large[Slope of Regression Line]

- Rate of change, "rise over run"   

- Predicted change in Y, for every 1-unit change in X



$$\LARGE
b_1 = r \times \frac{s_Y}{s_X}
$$

]]



---

.pull-leftsmall[
### Example 1)

**Predictor (IV)**  
*M* =  4.093   
*SD* = 1.31   

**Outcome (DV)**   
*M* = 12.290   
*SD* = 1.66   

**Correlation**   
*r* = .764   

]


.pull-rightbig[
```{r, echo=FALSE, dpi = 150, out.width = "90%"}
set.seed(843)
library(tidyverse)
df <- tibble::data_frame(
  x = rnorm(10) + 5,
  y = x + rnorm(10) + 10
) %>%
  mutate(pred = predict(lm(y ~ x, data = .)))

p_0 <- df %>%
  ggplot(aes(x, y)) +
    geom_point(size = 3) +
    scale_color_gradient2(low = "chartreuse3", 
                          mid = "grey90", 
                          high = "dodgerblue3") +
    theme(legend.position = "none") +
    coord_cartesian(ylim = c(11.9, 17.2),
                    xlim = c(2.5, 7.1)) +
  labs(x = "Independent Variable (IV)",
       y = "Dependent Variable (DV)") +
    theme_classic() +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme(text = element_text(size = 18)) 
  

p_0
```
]

.pull-left[
Scatterplot:

* 1 point per observation (person)
]


---
count: false

```{r, echo=FALSE, dpi = 150, out.width = "50%"}
p_1 <- p_0 +
    geom_vline(aes(xintercept = mean(x)),
               color = "darkorchid4",
               linetype = 2) +
    geom_hline(aes(yintercept = mean(y)),
               color = "darkorchid3",
               linetype = 2) +
  geom_text(label = "mean of Y",
               color = "darkorchid4",
            x = 2.75,
            y = 14.5)+
  geom_text(label = "mean of X",
               color = "darkorchid4",
            x = 4.5,
            y = 12)

p_1
```


.pull-left[
Scatterplot:

* 1 point per observation (person)
* Dashed lines = mean values
* Cross at the "Point of Averages"
]


---
count: false

```{r, echo=FALSE, dpi = 150, out.width = "50%"}
p_2 <- p_1 +
    geom_smooth(method = "lm",
                color = "coral2",
                se = FALSE,
                size = 1,
                alpha = .8) +
  geom_text(label = "Predicted Y\n based on X",
            color = "coral2",
            x = 6,
            y = 17)

p_2
```

.pull-left[
- Correlation = `r round(cor(df$x, df$y),3)`
- Slope = $b_1 = r \frac{s_y}{s_x} = .764 \frac{1.66}{1.31} = .968$
- Intercept = $b_0 = \bar{Y} - b_1 \bar{X} = 14.290 - (.968 * 4.093) = 10.328$
]



---
count: false

```{r, echo=FALSE, fig.ext='png', dpi = 150, out.width = "50%"}
p_2 +
    geom_segment(aes(xend = x, yend = mean(y)),
                 color = "chartreuse3") +
  geom_text(label = "Deviations of\nobserved Y's\nfrom the mean of Y",
            color = "chartreuse3",
            x = 6.5,
            y = 15.5)
```



.pull-left[
- Correlation = `r round(cor(df$x, df$y),3)`
- Slope = $b_1 = r \frac{s_y}{s_x} = .764 \frac{1.66}{1.31} = .968$
- Intercept = $b_0 = \bar{Y} - b_1 \bar{X} = 14.290 - (.968 * 4.093) = 10.328$
]

.pull-right[

$\LARGE SS_{total}$

]

---
count: false

```{r, echo=FALSE, fig.ext='png', dpi = 150, out.width = "50%"}
p_2 +
    geom_segment(aes(xend = x, yend = pred, y = mean(y)),
                 color = "dodgerblue3") +
  geom_text(label = "Deviations of\npredicted Y's\nfrom the mean of Y",
            color = "dodgerblue3",
            x = 6.5,
            y = 15.5)
```



.pull-left[
- Correlation = `r round(cor(df$x, df$y),3)`
- Slope = $b_1 = r \frac{s_y}{s_x} = .764 \frac{1.66}{1.31} = .968$
- Intercept = $b_0 = \bar{Y} - b_1 \bar{X} = 14.290 - (.968 * 4.093) = 10.328$
]



.pull-right[

$\LARGE SS_{total} = SS_{explained}$

]

---
count: false

```{r, echo=FALSE, fig.ext='png', dpi = 150, out.width = "50%"}
p_2 +
    geom_segment(aes(xend = x, yend = pred),
                 color = "firebrick3") +
  geom_text(label = "Deviations of\nobserved Y's\nfrom the predicted Ys",
            color = "firebrick3",
            x = 6.5,
            y = 15.5)
```



.pull-left[
- Correlation = `r round(cor(df$x, df$y),3)`
- Slope = $b_1 = r \frac{s_y}{s_x} = .764 \frac{1.66}{1.31} = .968$
- Intercept = $b_0 = \bar{Y} - b_1 \bar{X} = 14.290 - (.968 * 4.093) = 10.328$
]



.pull-right[

$\LARGE SS_{total} = SS_{explained} + SS_{unexplained}$

]

---
# Explaining Variance

$$\huge SS_{total} = SS_{explained} + SS_{unexplained}$$

Synonyms: 
* Explained = Regression
* Unexplained = Residual or Error


--

Coefficient of Determination: $\LARGE r^2 = \frac{\text{Explained Variation}}{\text{Total Variation}} = \frac{SS_{regression}}{SS_{total}}$

--

- Computed to determine how well regression equation predicts `Y` from `X`

--

- Range from 0 to 1

--

- SS divided by corresponding df gives us the Mean Square (Regression or Error)

--

- .nicegreen[The proportion of variance in the outcome "accounted for" or "attributable to" or "predictable from" or "explained by" the predictor]



---
# Again, Always **Visualize** Data First

### Scatterplots

```{r, echo=FALSE}
set.seed(843)
df <- tibble::data_frame(
  x = rnorm(100),
  y = x + rnorm(100)
)
```

.pull-left[

```{r, eval=FALSE, dpi = 150, out.width = "80%"}
library(tidyverse)
df %>%
  ggplot(aes(x, y)) +
    geom_point() +
    geom_smooth(se = FALSE,
                method = "lm")
```
]


--


.pull-right[
```{r, echo=FALSE, dpi = 150, out.width = "80%"}
library(tidyverse)
df %>%
  ggplot(aes(x, y)) +
    geom_point() +
    geom_smooth(se = FALSE,
                method = "lm")
```
]

---
## R Code: Regression

```{r, eval=FALSE}
df %>%
  lm(y ~ x,
     data = .) %>%
  summary()
```

--

```{r, echo=FALSE}
df %>%
  lm(y ~ x,
     data = .) %>%
  summary()
```


---
## R Code: Regression

```{r}
df %>%
  lm(y ~ x,
     data = .) %>%
  confint()
```

---
## R Code: Regression

```{r}
df %>%
  lm(y ~ x,
     data = .) %>%
  coef()
```

---
## R Code: Regression

```{r}
coef1 <- df %>%
  lm(y ~ x,
     data = .) %>%
  coef()
confint1 <- df %>%
  lm(y ~ x,
     data = .) %>%
  confint()
cbind(coef1, confint1)
```

---
## R Code: Predicted Values

```{r}
df %>%
  lm(y ~ x,
     data = .) %>%
  predict()
```


---
# Assumptions


- .nicegreen[Independence] of observations
  - SRS = simple random sample

--


- .nicegreen[Straight line] best fits data
  - linear, not curved, ect.

--


- .bluer[Y is **CONDITIONALLY** normally distributed, with constant variance,  around the line]
  - Does NOT apply to Y overall
  - DOES apply to Y values, FOR A GIVEN X value
  - Check with residual diagnostics
  - Does NOT apply to predictor variable(s) X --> Can be categorical or continuous
  


- Sampling .dcoral[distribution of the slope] ( $\huge b_1$ ) assumed normally distributed





---
background-image: url(figures/fig_assumptions_reg.png)
background-position: 50% 50%
background-size: 1200px

# Assumptions


---
# R Code: Assumption - Normal Residuals

```{r, dpi = 150, out.width = "50%"}
df %>%
  lm(y ~ x,
     data = .) %>%
  plot(which = 2)
```

---
# R Code: Assumptions - Normal Residuals

```{r, dpi = 150, out.width = "50%"}
df %>%
  lm(y ~ x,
     data = .) %>%
  resid() %>%
  hist()
```


---
class: inverse, center, middle

# Let's Apply This to the Cancer Dataset 


---
# Read in the Data

```{r}
library(tidyverse)    # Loads several very helpful 'tidy' packages
library(haven)        # Read in SPSS datasets
library(furniture)    # for tableC()
```

```{r, eval=FALSE}
cancer_raw <- haven::read_spss("cancer.sav")
```

```{r, include=FALSE}
cancer_raw <- haven::read_spss("data/cancer.sav")
```

### And Clean It

```{r, message=FALSE, warning=FALSE}
cancer_clean <- cancer_raw %>% 
  dplyr::rename_all(tolower) %>% 
  dplyr::mutate(id = factor(id)) %>% 
  dplyr::mutate(trt = factor(trt,
                             labels = c("Placebo", 
                                        "Aloe Juice"))) %>% 
  dplyr::mutate(stage = factor(stage))
```

---
## R Code: Regression

.pull-left[
```{r, eval=FALSE}
cancer_clean %>%
  lm(totalcin ~ age,
     data = .) %>%
  summary()
```

]

.pull-right[
```{r, echo=FALSE}
cancer_clean %>%
  lm(totalcin ~ age,
     data = .) %>%
  summary()
```
]

---
### R Code: Standardized

```{r}
cancer_clean %>%
  mutate(totalcinZ = scale(totalcin),
         ageZ = scale(age)) %>%
  lm(totalcinZ ~ ageZ, 
     data = .) %>%
  summary()
```


---
# R Code: Correlation vs. Standardized

.pull-left[
```{r, eval=FALSE}
cancer_clean %>%
  cor.test(~ totalcinZ + ageZ, 
           data = .)
```
]


.pull-right[
```{r, eval=FALSE}
cancer_clean %>%
  mutate(totalcinZ = scale(totalcin),
         ageZ = scale(age)) %>%
  lm(totalcinZ ~ ageZ, 
     data = .) %>%
  summary()
```
]

---
# R Code: Correlation vs. Standardized

.pull-left[
```{r, echo=FALSE}
cancer_clean %>%
  cor.test(~ totalcin + age, 
           data = .)
```
]


.pull-right[
```{r, echo=FALSE}
cancer_clean %>%
  mutate(totalcinZ = scale(totalcin),
         ageZ = scale(age)) %>%
  lm(totalcinZ ~ ageZ, 
     data = .) %>%
  summary()
```
]

---
class: inverse, center, middle

# Questions?


---
class: inverse, center, middle

# Next Topic

### Matched T-Test




---
class: inverse, center, middle

# Supplemental Slides


---
## Accuracy of Prediction

- All points do not fall on regression line
  - Prediction works for most, but not all in sample
  
--

- W/out knowledge of X, best prediction of Y is mean $\Large \bar{Y}$
  - $\Large s_y$ : best measure of prediction error

--

- With knowledge of X, best prediction of Y is from the equation $\Large \hat{Y}$
  - Standard error of estimate (SEE or $s_{Y \cdot X}$ ): best measure of prediction error
  - Estimated SD of residuals in population



---
# Accuracy of Prediction

.pull-left[
### .nicegreen[Standard Error of Estimate]
<br>

$$\large s_{Y \cdot X} = \sqrt{\frac{\sum (Y_i - \hat{Y})^2}{N - 2}} = \sqrt{\frac{SS_{residual}}{df}}$$

]


--

.pull-right[
### .dcoral[Residual or Error Variance <br> or Mean Square Error]

$$\large s_{Y \cdot X}^2 = \frac{\sum (Y_i - \hat{Y})^2}{N - 2} = \frac{SS_{residual}}{df}$$

]

--

<br>
.large[
$df = N - 2$ (2 df lost in estimating regression coefficients)

Seeking smallest $\LARGE s_{Y \cdot X}$ as it is a measure of variation of observations around regression line

]



---
# Standardized Coefficients (Beta weights)


- 1 SD-unit change in X represents a $\beta$ SD change in `Y`

--

- Intercept = 0 and is not reported when using $\beta$

--

- For simple regression only --> $r = \beta$ and $r^2 = \beta^2$
  - When raw scores transformed into z-scores: $r = b = \beta$

--
  
- Useful for variables with abstract unit of measure


